<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
    <head>
    <meta name="google-site-verification" content="eoPCGBBxDIK0Ff9Dk_dXsuHMTNzzSEZMbsfO4zriBK8" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <link rel="shortcut icon" href="./images/doge.ico">
	<meta name="keywords" content="Â∏∏Ëø™, Chang Di, Â∏∏Ëø™, CHANG Di, Â∏∏Ëø™, Di CHANG, University of Southern California, USC, Technical University of Munich, TUMÔºåDalian University of TechnologyÔºåDLUT, DUT, Â§ßËøûÁêÜÂ∑•Â§ßÂ≠¶, ÊÖïÂ∞ºÈªëÂ∑•‰∏öÂ§ßÂ≠¶, ÂçóÂä†Â∑ûÂ§ßÂ≠¶, M√ºnchen, Munich, ÊÖïÂ∞ºÈªë, California, Âä†Âà©Á¶èÂ∞º‰∫öÂ∑û">
	<meta name="description" content="Di CHANG&#39;s Home Page">
<!--    <link href="main.css" media="all" rel="stylesheet">-->
    <link rel="stylesheet" href="jemdoc.css" type="text/css">
    <title>Di CHANG | Â∏∏Ëø™</title>
    </head>

<body>


<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">
					<h1 style="color:#FF0000">Di (Kilian) Chang</h1><h1>
					<h1 style="color:#FF0000">Â∏∏Ëø™</h1><h1>
				</h1></div>

				<h3>CS Ph.D.  @ <a href="https://www.usc.edu/" target="_blank">University of Southern California</a></h3>
				<h3>Research Scientist Intern @ <a href="https://www.tiktok.com/en/" target="_blank">TikTok</a></h3>
<!--                <h3>University of Southern California</h3>-->
				<p>
					<a href="https://www.cs.usc.edu/" target="_blank">Department of Computer Science</a> <br>
					<a href="https://viterbischool.usc.edu/" target="_blank">Viterbi School of Engineering</a><br>
					<a href="https://www.usc.edu/" target="_blank">University of Southern California</a> <br>

<!--					Rm B06, Hedco Neurosciences Building, 3641 Watt Way, Los Angeles, CA 90089-2520, USA <br>-->

					Email1: dichang at usc dot edu (primary) <br>
					Email2: dchang at ict dot usc dot edu <br>
					Email3: di dot chang at bytedance dot com
				</p>
				<p> <a href="https://scholar.google.com.hk/citations?hl=en&user=68wkMTgAAAAJ" target="_blank"><img src="./pics/google_scholar3.png" height="40px" style="margin-bottom:-3px"></a>
					<a href="https://github.com/Boese0601" target="_blank"><img src="./pics/github_s.jpg" height="40px" style="margin-bottom:-3px"></a>
					<a href="https://twitter.com/DiChang10" target="_blank"><img src="./images/twitter.png" height="40px" style="margin-bottom:-3px"></a>
                    <a href="https://www.linkedin.com/in/di-chang-004784206/" target="_blank"><img src="./pics/LinkedIn2.png" height="40px" style="margin-bottom:-3px"></a>
                    <a href="files/CV_Di.pdf"><img src="./pics/cv2.png" height="40px" style="margin-bottom:-3px"></a>
					&nbsp &nbsp
					<!-- <a href="#C1">[<font <font size="3" color="#CB4335"><b>About Me</b></font>] </a> -->
					<a href="#C2">[<font size="3" color="#CB4335"><b>News</b></font>]</a>
					<a href="#edu">[<font size="3" color="#CB4335"><b>Education</b></font>]</a>
					<a href="#C3">[<font size="3" color="#CB4335"><b>Publications</b></font>]</a>
					<a href="#C4">[<font size="3" color="#CB4335"><b>Experience</b></font>]</a></li>
				</p>
			</td>
			<td>
				<img src="./images/dichang.png" border="0" width="230"><br>
<!--				<img src="pics/cover.png" border="0" width="540"><br>-->
			</td>
		</tr><tr>
	</tr></tbody>
</table>

<!--<style>-->
<!--ul-->
<!--{-->
<!--	list-style-type:none;-->
<!--	margin:0;-->
<!--	padding:0;-->
<!--}-->
<!--li a:hover {-->
<!--    background-color: #555;-->
<!--    color: white;-->
<!--}-->

<!--</style>-->


<!--   #0F73B6-->

<h2><a id="C1" ><font color="#CB4335">About Me</font></a></h2>
<p> <a href="https://www.cs.usc.edu/"><img src="./pics/USC_logo.png" height="100px" style="margin-bottom:-3px"></a>&nbsp &nbsp &nbsp
	<a href="https://www.tiktok.com/en/"><img src="./images/TikTok.jpeg" height="100px" style="margin-bottom:-3px"></a>&nbsp &nbsp &nbsp
	<a href="https://www.tum.de/en/"><img src="./images/TUM.jpg" height="100px" style="margin-bottom:-3px"></a>&nbsp &nbsp &nbsp
	<a href="https://www.epfl.ch/en/"><img src="./images/EPFL.png" height="120px" style="margin-bottom:-3px"></a>&nbsp &nbsp &nbsp &nbsp
	<a href="https://hkust.edu.hk/home"><img src="./images/HKUST.jpg" height="100px" style="margin-bottom:-3px"></a>&nbsp &nbsp &nbsp
	<br>
	<br>
	<!-- <a href="https://www.microsoft.com/en-us/research/"><img src="./pics/microsoft_logo.jfif" height="95px" style="margin-bottom:-3px"></a>&nbsp &nbsp &nbsp &nbsp
	<a href="https://www.uii-ai.com/en/"><img src="./pics/UII_logo33.png" height="98px" style="margin-bottom:-3px"></a>&nbsp &nbsp &nbsp
	<a href="http://flexiv.com/"><img src="./pics/flexiv_logo.jfif" height="105px" style="margin-bottom:-3px"></a> -->

</p>
<p>I am a <strike>first-year</strike> second-year Ph.D. student in the Department of Computer Science at <a  href="https://www.cs.usc.edu/">University of Southern California</a> (USC) with Prof. <a  target="_blank" href=https://people.ict.usc.edu/~soleymani/ target="_blank" rel="external">Mohammad Soleymani</a> of <a  href="https://www.ihp-lab.org/">IHP-Lab</a>.
	I'm also currently a Research Intern at TikTok-ByteDance AI Lab US, Intelligent Creation Team, working with Dr. <a  target="_blank" href=https://seasonsh.github.io/ rel="external"> Yichun Shi </a> and Dr. <a target="_blank" href= https://scholar.google.com/citations?user=_MAKSLkAAAAJ&hl=en  rel="external">Xiao Yang</a>.
	<!-- I work closely with Prof. <a  target="_blank" href=https://xiaolonw.github.io/ target="_blank" rel="external"> Xiaolong Wang </a> from <a  target="_blank" href=https://ucsd.edu/ target="_blank" rel="external"> UC San Diego </a>. -->
</p>
	Before that, I spent four wonderful years during my undergraduate at <a href="https://en.dlut.edu.cn/">Dalian University of Technology</a> and <a href=https://www.tum.de/en/ target="_blank" >Technical University of Munich</a>, studying Informatics and Information Engineering.
	I'm honored to be advised by Prof. <a  target="_blank" href=https://www.niessnerlab.org/members/matthias_niessner/profile.html target="_blank" rel="external"> Matthias Niessner </a> and
	Dr. <a  target="_blank" href=https://aljazbozic.github.io/ target="_blank" rel="external"> Alja≈æ Bo≈æiƒç </a> during my praktikum at <a  target="_blank" href="https://www.niessnerlab.org/" target="_blank" rel="external">Visual Computing Group</a>.
	I spent a wonderful summer in 2022 with Dr. <a  target="_blank" href=https://sites.google.com/view/tong-zhang rel="external"> Tong Zhang </a>
	and Prof. <a  target="_blank" href=https://people.epfl.ch/sabine.susstrunk?lang=en target="_blank" rel="external"> Sabine S√ºsstrunk</a> at EPFL CS and in 2021 with Prof. <a  target="_blank" href=https://www.danxurgb.net/index.html target="_blank" rel="external"> Dan Xu </a> at HKUST CSE.
</p>
	I'm interested in Computer vision, Multi-View Stereo and Neural Rendering. My current research focuses include:
<ul>
	  <li>3D Vision, especially Multi-View Geometry and Scene Reconstruction with Deep Learning approaches. </li>
	  <li>Video Generation and View Synthesis with Generative Models(DDPM).</li>
	  <li>Multimodal Machine Learning.</li>
	  <li>Facial Expression Analysis and Affective Computing.</li>
<!--	  <li>Effortless AI (how generative models reduce human effort and boost discriminative models)</li>-->
	</ul>
<!--<ul>-->
<!--	  <li>Causal Explainable AI ((1) Understanding reasoning logic and causality of Neural Networks (NN) (2) Use explanation as feedback to help improve the performance of the original NN.) </li>-->
<!--	  <li>Interpretable human-AI interaction (understanding AI models beyond accuracy, such as disentangled representation learning, human-NN knowledge exchange, steerability, generalization, fairness and bias)</li>-->
<!--	  <li>Humanoid Neural Network (simulating human cognitive learning ability (Imagination, Reasoning, Visual Recognition) by using various learning algorithms (Generative models, Representation Learning, Graph Neural Network, Contrastive Learning, etc.)</li>-->
<!--	  <li>Effortless AI (how generative models reduce human effort and boost discriminative models)</li>-->
<!--	</ul>-->

<!--	I'm interested in Machine Learning, Computer vision, and their applications towards Artificial General Intelligence (AGI). My current research focuses include:-->
<!--<ul>-->
<!--	  <li>interpretable human-AI interaction (interpretability, steerability, disentangled representation learning)</li>-->
<!--	  <li>generative models (data augmentation, how generative models boost discriminative models)</li>-->
<!--	  <li>graph neural networks (structure and relationship learning)</li>-->
<!--	  <li>visual reasoning, attention and saliency (cognitive learning, eye tracking)</li>-->
<!--	</ul>-->
<!--<p>My research interests lie in Machine Learning, Computer vision, and AGI. Currently, I am focusing on simulating Cognitive Baby Learning (Imagination, Reasoning, Attention)-->
<!--by using various learning algorithms (Representation Learning, Generative models, GNN, Reinforcement Learning,  Meta-Learning, etc.).</p>-->
<p> <b>Research opportunities</b>:<br>
	For USC undergraduate students: We have openings for interns through <a href=https://viterbiundergrad.usc.edu/research/curve/ target=_blank rel=noopener>CURVE</a> program. Usually we recruit students (first-time researchers and continuing researchers) during Fall and Spring semesters, please apply early. <br><br>
	For USC master students: Please directly email Prof. <a  target="_blank" href=https://people.ict.usc.edu/~soleymani/ target="_blank" rel="external">Mohammad Soleymani</a> for such inquiries. It would be better if you've attended CSCI 535 and obtained a good grade. <br><br>
	For all other students outside USC: we usually don't have openings during Fall and Spring semesters, but Prof. <a  target="_blank" href=https://people.ict.usc.edu/~soleymani/ target="_blank" rel="external">Mohammad Soleymani</a> sometimes recruits good candidates (Undergrad/Ms/visiting PhD) during the Summer. Email him for further details. <br><br>
	I am happy to collaborate and/or answer questions about my research and my previous study at TUM and current PhD program at USC. If you are interested in research collaboration or have any inquiries about my experience, please send me an email.
</p>





<h2><a id="C2" ><font color="#CB4335">News</font></a></h2>
<ul>

<div style="height:200px;width:fit-content;overflow:auto;background:#FFFFFF;">
	<li>
		<p>[2022/08/14] Our <a href=https://boese0601.github.io/fgnet/ target=_blank rel=noopener>FG-Net</a> and <a href=https://boese0601.github.io/fgnet/ target=_blank rel=noopener>LibreFace</a> has been accepted by WACV 2024! Find me at Waikoloa, Hawaii.</p>
	</li>
	<li>
		<p>[2023/05/22]  Start my Summer Internship at <a href="https://www.tiktok.com/en/">TikTok</a> (San Jose Office in CA). Looking forward to building connections with talents in the Bay Area! </p>
	</li>
	<li>
		<p>[2022/08/19]  Today I start my Ph.D. at <a href="https://www.cs.usc.edu/">USC Viterbi Computer Science</a>! Fight on, Trojans!  </p>
	</li>
	<li>
		<p>[2022/07/04]  My first single-first-author paper <a href=https://www.niessnerlab.org/projects/chang2022rcmvsnet.html target=_blank rel=noopener>RC-MVSNet</a> has been accepted by ECCV 2022! See you in Tel Aviv!</p>
	</li>
	<li>
		<p>[2022/04/08]  I'm joining <a href=https://www.ihp-lab.org/ target=_blank rel=noopener>IHP-Lab</a> at <a href=https://www.cs.usc.edu/ target=_blank rel=noopener>USC</a> as a PhD student, see you in Los Angeles!</p>
	</li>
	<li>
		<p>[2022/03/15] Our <a href=https://mizhenxing.github.io/gbinet/ target=_blank rel=noopener>GBi-Net</a> has been accepted by CVPR 2022!</p>
	<li>
		<p>[2022/03/07] I will join <a href=https://www.3dunderstanding.org target=_blank rel=noopener>3D AI Lab</a> at <a href=https://www.tum.de/en/ target=_blank rel=noopener>Technische Universit√§t M√ºnchen</a> as a guided research student with Professor Angela Dai.</p>
	</li>
	<li>
		<p>[2022/01/15] I will join <a href=https://www.epfl.ch/labs/ivrl/ target=_blank rel=noopener>Image and Visual Representation Lab</a> at <a href=https://www.epfl.ch/en/ target=_blank rel=noopener>√âcole Polytechnique F√©d√©rale de Lausanne</a> as a research intern/visiting researcher.
	</li>
	<li>
		<p>[2021/10/15] I will join <a href=https://niessnerlab.org/ target=_blank rel=noopener>Visual Computing & Artificial Intelligence Lab</a> at <a href=https://www.tum.de/en/ target=_blank rel=noopener>Technische Universit√§t M√ºnchen</a> with Professor Matthias Niessner.
	</li>
	<li>
		<p>[2021/03/04] I will join <a href=https://www.danxurgb.net/index.html target=_blank rel=noopener>Prof.Xu&rsquo;s vision group</a> at <a href=https://hkust.edu.hk/home target=_blank rel=noopener>HKUST</a> as a research intern during summer 2021.
	</li>
	

</div>
</ul>
<br>



<h2><a id="edu" ><font color="#CB4335">Education</font></a></h2>
<table>
	<tbody>
		<tr>
			<td width="800">
 
			<h4> University of Southern California, Los Angeles, USA <br>(Aug. 2022 - Present)</h4>
			<ul>
				<li>
				<b>Doctor of Philosophy in Computer Science</b></li>
				<li>Major Orientation: Deep Learning for 3D Vision and Affective Computing </li>
			</ul>
			</td>
<td>
	<img id="school_logo" src="./images/usc_cs_logo.jpeg" height="100px">
</td>
</tr>
<tr></tr>
</tbody></table>



<table>
	<tbody>
		<tr>
			<td width="800">
 
 <h4> Technical University of Munich, Munich, Germany (Aug. 2021 - Jul. 2022)</h4>
 <ul>
	<li>
	  <b>Bachelor of Science in Informatics</b></li>
	<li>Major Orientation: Deep Learning for 3D Perception, 3D Scanning and Spatial Learning </li>
	<li>Overall GPA: 1.2/1.0 </li>
	<li>Bachelor thesis: "Supervised and Unsupervised Multi View Stereo for Depth Inference"</li>
 </ul>
</td>
<td>
	<img id="school_logo" src="./images/TUM.jpg" height="100px">
</td>
</tr>
<tr></tr>
</tbody></table>

<table>
<tbody>
	<tr>
		<td width="800">
 <h4> Dalian University of Technology, Dalian, China (Sep. 2018 - Jul. 2021)</h4>
 <ul>
	<li>
	  <b>Bachelor of Engineering in Eletronic Information Engineering</b></li>
	<li>Major Orientation: Object Detection and Tracking</li>
	<li>Overall GPA: 91.5/100 | 3.93/4.0</li>
 </ul>
</td>
<td>
	<img id="school_logo" src="./images/DUT.jpg" height="100px">
</td>
</tr>
<tr></tr>
</tbody></table>


<h2><a id="C3" ><font color="#CB4335">Selected Publications</font></a> [<a href=https://scholar.google.com.hk/citations?hl=en&user=68wkMTgAAAAJ>Google Scholar</a>]</h2>

<table id="tbPublications" width="100%">


<!-- 
	<tr>
		<td width="306">
		<img src="pics/VisualReasoning.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td>
			<b>A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts</b><br>
			<p style="color:#2E86C1 ";> <b>A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts</b> </p><br>
		<br>
			<b>TL;DR:</b> Takes a step towards mimicking the reasoning process of NNs and provide logical, concept-level
						explanations for final model decisions. <br>
			<br>
			<b>Yunhao Ge</b>, Yao Xiao, Zhi Xu, Meng Zheng, Srikrishna Karanam, Terrence Chen, Laurent Itti and Ziyan Wu  <br>
		<em>IEEE/ CVF International Conference on Computer Vision and Pattern Recognition </em>(<i><b>CVPR</b></i>), 2021.
		<p></p>
			<p>[<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ge_A_Peek_Into_the_Reasoning_of_Neural_Networks_Interpreting_With_CVPR_2021_paper.pdf" target="_blank">paper</a>]
			[<a href="https://github.com/gyhandy/Visual-Reasoning-eXplanation" target="_blank">github</a>]
			[<a href="http://ilab.usc.edu/andy/vrx" target="_blank">website</a>]
			[<a href="https://youtu.be/ZzkpUrK-cRA" target="_blank">video</a>]
			[<a href="https://zhuanlan.zhihu.com/p/380264276" target="_blank">Áü•‰πé</a>]
			[<a href="https://mp.weixin.qq.com/s/FhQsi7twHkGskcE5fshOxA" target="_blank">Êú∫Âô®‰πãÂøÉ</a>]
			[<a href="https://mp.weixin.qq.com/s/DqYlLVMggNEWuN76-OXJmQ" target="_blank">AIÁßëÊäÄËØÑËÆ∫</a>]
			</p>
		</td>
	</tr>




	<tr>
		<td width="306">
		<img src="pics/GSL3.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td> <b>Zero-shot Synthesis with Group-Supervised Learning</b> <br>
		<br>
		<b>TL;DR:</b> Using Controllable disentangled representation learning to simulate human knowledge factorization for imagination <br>
			<br>
		<b>Yunhao Ge</b>, Sami Abu-El-Haija, Gan Xin and Laurent Itti  <br>
		<em>International Conference on Learning Representations </em>(<i><b>ICLR</b></i>), 2021.
		<p></p>
		<p>[<a href="https://arxiv.org/pdf/2009.06586.pdf" target="_blank">paper</a>]
			[<a href="https://github.com/gyhandy/Group-Supervised-Learning" target="_blank">code</a>]
			[<a href="http://sami.haija.org/iclr21gsl" target="_blank">website</a>]
			[<a href="http://ilab.usc.edu/datasets/fonts" target="_blank">Fonts Dataset</a>]
			[<a href="https://viterbischool.usc.edu/news/2021/07/usc-researchers-enable-ai-to-use-its-imagination/" target="_blank">USC Viterbi Press</a>]
			[<a href="https://zhuanlan.zhihu.com/p/364895887" target="_blank">Áü•‰πé</a>]
			[<a href="https://mp.weixin.qq.com/s/o2HBYf3NF3UsMxUqkASdyg" target="_blank">AIÁßëÊäÄËØÑËÆ∫</a>]<br>
			[<a  target="_blank" href=https://viterbischool.usc.edu/news/2021/07/usc-researchers-enable-ai-to-use-its-imagination/ target="_blank" rel="external"> USC News </a>]
			[<a  target="_blank" href=https://techxplore.com/news/2021-07-enabling-artificial-intelligence.html target="_blank" rel="external"> Tech Xplore </a>]
			[<a  target="_blank" href=https://www.technologynetworks.com/informatics/news/enabling-ai-to-use-its-imagination-350886 target="_blank" rel="external"> Technology Networks </a>]
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>

			<tr>
		<td width="306">
		<img src="pics/CIR.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td>
			<b>Encouraging Disentangled and Convex Representation with Controllable Interpolation Regularization</b><br>
			<p style="color:#2E86C1 ";> <b>A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts</b> </p><br>
		<br>
		<b>TL;DR:</b> A simple yet efficient method: Controllable Interpolation Regularization (CIR), which creates a positive loop where the disentanglement and convexity can help each other  <br>
			<br>
			<b>Yunhao Ge</b>, Zhi Xu, Yao Xiao, Gan Xin, Yunkui Pang and Laurent Itti <br>
		<em>arXiv, 2021.
		<p></p>
		<p>[<a href="https://arxiv.org/pdf/2112.03163.pdf" target="_blank">paper</a>]</p>
		</td>
	</tr>

	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>




		<tr>
		<td width="306">
		<img src="pics/GraphAE.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td>
			<b>Graph Autoencoder for Graph Compression and Representation Learning</b><br>
			<p style="color:#2E86C1 ";> <b>A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts</b> </p><br>
		<br>
		<b>TL;DR:</b> Multi-kernel Inductive Attention Graph Autoencoder (MIAGAE) utilizes the node similarity and graph structure to compress all nodes and edges as a whole.  <br>
			<br>
			<b>Yunhao Ge<sup>*</sup></b>, Yunkui Pang<sup>*</sup>, Linwei Li and Laurent Itti (*=equal contribution)<br>
		<em>Neural Compression: From Information Theory to Applications--Workshop@ </em>(<i><b>ICLR</b></i>), 2021.
		<p></p>
		<p>[<a href="https://openreview.net/pdf?id=Bo2LZfaVHNi" target="_blank">paper</a>]
			[<a href="https://github.com/Pangyk/Graph_AE" target="_blank">code</a>]
			[<a href="http://ilab.usc.edu/datasets/i2sg" target="_blank">Img2SceneGraph</a>]</p>
			<p><strong style="color:blue">Spotlight Presentation</strong></p>
		</td>
	</tr>


    <tr>
		<td width="306">
		<img src="pics/eccv.gif" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td> <b>Pose Augmentation: Class-agnostic Object Pose Transformation for Object Recognition</b> <br>
			<br>
			<b>TL;DR:</b> Eliminate-add generator to explicitly disentangle pose from object identity by maximizing pose entropy<br>
			<br>
		<b>Yunhao Ge</b>, Jiaping Zhao, Laurent Itti  <br>
		<em>European Conference on Computer Vision</em> (<i><b>ECCV</b></i>), 2020.
		<p></p>
		<p>[<a href="https://arxiv.org/pdf/2003.08526.pdf" target="_blank">paper</a>]
			[<a href="https://github.com/gyhandy/Pose-Augmentation" target="_blank">github</a>]
			[<a href="https://youtu.be/WHAFj9KXRFY" target="_blank">video-1min</a>]
			[<a href="https://youtu.be/9N8eyOmCWh4" target="_blank">video-10min</a>]</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>

	<tr>
		<td width="306">
		<img src="pics/Beneficial.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td> <b>Beneficial Perturbation Network for designing general adaptive artificial intelligence systems</b> <br>
		<b>TL;DR:</b>Allowing a single network to learn potentially unlimited parallel input to output mappings, and to switch on the fly
			between them at runtime.<br>
			<br>
		Shixian Wen, Amanda Rios<sup>*</sup>, <b>Yunhao Ge<sup>*</sup></b> and Laurent Itti (*=equal contribution) <br>
		<em> IEEE Transactions on Neural Networks and Learning Systems </em>(<i><b>TNNLS</b></i>), 2021.
		<p></p>
		<p>[<a href="https://arxiv.org/pdf/2009.13954.pdf" target="_blank">paper</a>]
		</td>




    <tr>
		<td width="306">
		<img src="pics/project11.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td> <b>Unpaired MR to CT Synthesis with Explicit Structural Constrained Adversarial Learning</b> <br>
			 <br>
		<b>Yunhao Ge<sup>*</sup></b>, Dongming Wei<sup>*</sup>, Zhong Xue, Yiqiang Zhan, Xiang Zhou, Qian Wang and Shu Liao (*=equal contribution)<br>
		<em>IEEE International Symposium on Biomedical Imaging</em> (<i><b>ISBI</b></i>), 2019.
		<p></p>
		<p>[<a href="files/UNPAIRED.pdf" target="_blank">paper</a>]
            [<a href="https://github.com/gyhandy/Unpaired-Cross-modality-Image-Synthesis" target="_blank">code</a>]</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr> -->

    <tr>
		<td width="306">
		<img src="images/libre.jpg" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td> <b>LibreFace: An Open-Source Toolkit for Deep Facial Expression Analysis</b> <br>
			<br>
			<b>Di Chang</b>, <a href=https://yufengyin.github.io/ target=_blank rel=noopener>Yufeng Yin</a>, Zongjian Li, <a href="https://scholar.google.com/citations?user=HuuQRj4AAAAJ&hl=en" target=_blank rel=noopener>Minh Tran</a>, and <a href=https://people.ict.usc.edu/~soleymani/ target=_blank rel=noopener>Mohammad Soleymani</a><br>
		<em>IEEE/CVF Winter Conference on Applications of Computer Vision</em> (<i><b>WACV</b></i>), 2024.  <b>(Application Track)</b>
		<p></p>
		<p>
			[<a href="https://boese0601.github.io/libreface/" target="_blank">project page</a>]
			[<a href="https://arxiv.org/abs/2308.10713" target="_blank">paper</a>]
			[<a href="https://github.com/ihp-lab/LibreFace" target="_blank">code</a>]
			[<a href="" target="_blank">video</a>]
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>

    <tr>
		<td width="306">
		<img src="images/fgnet.jpg" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td> <b>FG-Net: Facial Action Unit Detection with Generalizable Pyramidal Features</b> <br>
			<br>
			<a href="https://yufengyin.github.io/" target=_blank rel=noopener>Yufeng Yin</a>, <b>Di Chang</b>,  <a href="https://guoxiansong.github.io/homepage/index.html" target=_blank rel=noopener>Guoxian Song</a>, <a href="https://ssangx.github.io/" target=_blank rel=noopener>Shen Sang</a>, <a href="https://tiancheng-zhi.github.io/" target=_blank rel=noopener>Tiancheng Zhi</a>, <a href="https://scholar.google.com/citations?user=fv8F6CEAAAAJ&hl=en" target=_blank rel=noopener>Jing Liu</a>, <a href="http://linjieluo.com/" target=_blank rel=noopener>Linjie Luo</a>, and <a href=https://people.ict.usc.edu/~soleymani/ target=_blank rel=noopener>Mohammad Soleymani</a><br>
		<em>IEEE/CVF Winter Conference on Applications of Computer Vision</em> (<i><b>WACV</b></i>), 2024.  <b>(Algorithms Track)</b>
		<p></p>
		<p>
			[<a href="" target="_blank">project page</a>]
			[<a href="" target="_blank">paper</a>]
			[<a href="" target="_blank">code</a>]
			[<a href="" target="_blank">video</a>]
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>

    <tr>
		<td width="306">
		<img src="images/rc.jpg" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td> <b>RC-MVSNet: Unsupervised Multi-View Stereo with Neural Rendering</b> <br>
			<br>
			<b>Di Chang</b>, <a href=https://aljazbozic.github.io/ target=_blank rel=noopener>Alja≈æ Bo≈æic</a>, <a href="https://people.epfl.ch/tong.zhang?lang=en" target=_blank rel=noopener>Tong Zhang</a>, Qingsong Yan, Yingcong Chen, <a href=https://people.epfl.ch/sabine.susstrunk target=_blank rel=noopener>Sabine S√ºsstrunk</a> and <a href=https://niessnerlab.org/ target=_blank rel=noopener>Matthias Nie√üner</a><br>
			<!-- <b>Di Chang</b>, <a>Alja≈æ Bo≈æiƒç, Tong Zhang, Qingsong Yan, Yingcong Chen, Sabine S√ºsstrunk and Matthias Nie√üner <br> -->
		<em>European Conference on Computer Vision</em> (<i><b>ECCV</b></i>), 2022.
		<!-- <em>Arxiv Preprint</em> (<i><b>Under Review</b></i>), 2022. -->
		<p></p>
		<p>
			[<a href="https://boese0601.github.io/rc-mvsnet/" target="_blank">project page</a>]
			[<a href="https://arxiv.org/abs/2203.03949" target="_blank">paper</a>]
			[<a href="https://github.com/Boese0601/RC-MVSNet" target="_blank">code</a>]
			[<a href="https://www.youtube.com/watch?v=I_Q47TxTLbs" target="_blank">video</a>]
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>

    <tr>
		<td width="306">
		<img src="images/gbinet.JPG" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td> <b>Generalized Binary Search Network for Highly-Efficient Multi-View Stereo</b> <br>
			<br>
		<a href="https://mizhenxing.github.io/">Zhenxing Mi</a>, <b>Di Chang</b> and <a href="https://www.danxurgb.net/index.html">Dan Xu</a> <br>
		<em>IEEE/ CVF International Conference on Computer Vision and Pattern Recognition</em> (<i><b>CVPR</b></i>), 2022.
		<p></p>
		<p>
			[<a href="https://mizhenxing.github.io/gbinet/" target="_blank">project page</a>]
			[<a href="https://arxiv.org/abs/2112.02338" target="_blank">paper</a>]
            [<a href="https://github.com/MiZhenxing/GBi-Net" target="_blank">code</a>]
		</p>

		<!-- <p><strong style="color:blue">Oral Presentation</strong></p> -->
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>


<!--    <tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/project3.png" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td> <b>A Real-time Gesture Prediction System Using Neural Networks and Multimodal Fusion-->
<!--					based on Data Glove</b> <br>-->
<!--			<br>-->
<!--		<b>Yunhao Ge</b>, Bin Li and Weixin Yan <br>-->
<!--		<em>IEEE International Conference on Advanced Computational Intelligence</em> (<i><b>ICACI</b></i>), 2018.-->
<!--		<p></p>-->
<!--		<p>[<a href="files/Real.pdf" target="_blank">paper</a>]</p>-->
<!--			<p><strong style="color:blue">Oral Presentation</strong></p>-->
<!--		</td>-->
<!--	</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->

<!--    <tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/HHnet.png" width="285px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td> <b>HH-Net: Image driven microscope fast auto-focus with deep neural network</b><br>-->
<!--			<br>-->
<!--		<b>Yunhao Ge</b>, Bin Li, Yanzheng Zhao and Weixin Yan <br>-->
<!--		<em>International Conference on Biomedical Engineering and Technology</em> (<i><b>ICBET</b></i>), 2019.-->
<!--		<p></p>-->
<!--		<p>[<a href="files/HH-net.pdf" target="_blank">paper</a>]-->
<!--        </p>-->
<!--		</td>-->
<!--	</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->

<!--    <tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/project4.png" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td> <b>Melanoma Segmentation and Classification in Clinical Images Using Deep Learning</b><br>-->
<!--			<br>-->
<!--		<b>Yunhao Ge</b>, Bin Li and Weixin Yan <br>-->
<!--		<em>ACM International Conference on Machine Learning and Computing</em> (<i><b>ICMLC</b></i>), 2018.-->
<!--		<p></p>-->
<!--		<p>[<a href="files/Melanoma.pdf" target="_blank">paper</a>][<a  target="_blank" href="https://dl.acm.org/citation.cfm?id=3195164">Paper Link</a>]-->
<!--        </p>-->
<!--		</td>-->
<!--	</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->

<!--    <tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/project5.png" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td> <b>Benign and Malignant Mammographic Image Classification based on Convolutional-->
<!--					Neural Networks</b><br>-->
<!--			<br>-->
<!--		Bin Li, <b>Yunhao Ge</b>, Yanzheng Zhao, Enguang Guan and Weixin Yan <br>-->
<!--		<em>ACM International Conference on Machine Learning and Computing</em> (<i><b>ICMLC</b></i>), 2018.-->
<!--		<p></p>-->
<!--		<p>[<a href="files/Benign.pdf" target="_blank">paper</a>][<a  target="_blank" href="https://dl.acm.org/citation.cfm?id=3195163&dl=ACM&coll=DL">Paper Link</a>]-->
<!--        </p>-->
<!--		</td>-->
<!--	</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->

<!--    <tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/project.png" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td> <b>Effect of Mechanical Error on Dual-Wedge Laser Scanning System and Error Correction</b><br>-->
<!--			<br>-->
<!--		<b>Yunhao Ge</b>, Jihao Liu, Fenfen Xue, Enguang Guan, Weixin Yan and Yanzheng Zhao <br>-->
<!--		<i><b>Applied Optics</b></i>, 2018.-->
<!--		<p></p>-->
<!--		<p>[<a href="files/Effect.pdf" target="_blank">paper</a>][<a  target="_blank" href="https://www.osapublishing.org/ao/abstract.cfm?uri=ao-57-21-6047">Paper Link</a>]-->
<!--        </p>-->
<!--		</td>-->
<!--	</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->

<!--    <tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/project8.png" width="285px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td> <b>Dynamic Drive Performances of the Bionic Suction Cup Actuator Based on Shape Memory Alloy</b><br>-->
<!--		<b>Yunhao Ge</b>, Jihao Liu, Bin Li, Weixin Yan and Yanzheng Zhao  <br>-->
<!--		<em>Intelligent Robotics and Applications</em> (<i><b>ICIRA</b></i>), 2017.-->
<!--		<p></p>-->
<!--		<p>[<a href="files/Dynamic.pdf" target="_blank">paper</a>][<a  target="_blank" href="https://link.springer.com/chapter/10.1007%2F978-3-319-65289-4_2">Paper Link</a>]-->
<!--        </p>-->
<!--		</td>-->
<!--	</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->


<!--	<tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/project7.png" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td><strong>Interactive Experience across Modern Age and Tradition: Application of Arduino in Shadow Play</strong><br>-->
<!--            <em>Kun Yin, <b>Yunhao Ge</b>, Heshan Liu and Yongquan Yin  <br></em>-->
<!--            <em>Advances in Mechatronics and Machinery </em>.-->
<!--			<br>[<a target="_blank" href="files/Interactive.pdf">PDF</a>] [<a  target="_blank" href="https://www.scientific.net/AMM.868.242">Paper Link</a>]-->
<!--&lt;!&ndash;		<br>[<a  target="_blank" href="https://www.scientific.net/AMM.868.242">Paper Link</a>]&ndash;&gt;-->
<!--        <br><br>-->
<!--		</td>-->
<!--	</tr>-->



</table>
<br>
<h2><a id="C4" ><font color="#CB4335">Intern & Work Experience</font></a></h2>

<table>
	<tbody>
		<tr>
			<td width="800">
<h4>TikTok, Los Angeles, CA, USA (Aug. 2023 - Nov. 2023) </h4>
	  <ul>
        <li>Position: Research Scientist Intern (Part-time) at ByteDance AI Lab US</li>
		  <li>Supervisor: Dr. <a  target="_blank" href=https://seasonsh.github.io/ rel="external"> Yichun Shi </a>, Dr. <a  target="_blank" href=https://guoxiansong.github.io/homepage/index.html rel="external"> Guoxian Song </a>, Dr. <a  target="_blank" href=https://hongyixu37.github.io/homepage/ rel="external"> Hongyi Xu </a>, and Dr. <a  target="_blank" href=https://mai-t-long.com/ target="_blank" rel="external"> Long Mai</a></li>
        <li>Project: Half-body Motion Transfer with Diffusion Models </li>
    </ul>
</td>
<td>
	<img id="school_logo" src="./images/TikTok.jpeg" height="100px">
</td>
</tr>
<tr></tr>
</tbody></table>

<table>
	<tbody>
		<tr>
			<td width="800">
<h4>TikTok, San Jose, CA, USA (May. 2023 - Aug. 2023) </h4>
	  <ul>
        <li>Position: Research Scientist Intern at ByteDance AI Lab US</li>
		  <li>Supervisor: Dr. <a  target="_blank" href=https://seasonsh.github.io/ rel="external"> Yichun Shi </a>, Dr. <a  target="_blank" href=https://guoxiansong.github.io/homepage/index.html rel="external"> Guoxian Song </a>, Dr. <a  target="_blank" href=https://hongyixu37.github.io/homepage/ rel="external"> Hongyi Xu </a>, and Dr. <a  target="_blank" href=https://mai-t-long.com/ target="_blank" rel="external"> Long Mai</a></li>
        <li>Project: Text-guided Half-body Image Editing with Diffusion Models </li>
    </ul>
</td>
<td>
	<img id="school_logo" src="./images/TikTok.jpeg" height="100px">
</td>
</tr>
<tr></tr>
</tbody></table>

<table>
	<tbody>
		<tr>
			<td width="800">
<h4>EPFL, Lausanne, Switzerland (Jul. 2022 - Oct. 2022) </h4>
	  <ul>
        <li>Position: Summer@EPFL (Top 2% from the candidates in the world) hired by <a  target="_blank" href=https://www.epfl.ch/labs/ivrl/ target="_blank" rel="external">IVRL</a></li>
		  <li>Supervisor: Dr. <a  target="_blank" href=https://sites.google.com/view/tong-zhang rel="external"> Tong Zhang </a>
			and Prof. <a  target="_blank" href=https://people.epfl.ch/sabine.susstrunk?lang=en target="_blank" rel="external"> Sabine S√ºsstrunk</a></li>
        <li>Project: Video Synthesis with Diffusion Models </li>
    </ul>
</td>
<td>
	<img id="school_logo" src="./images/EPFL.png" height="100px">
</td>
</tr>
<tr></tr>
</tbody></table>
<!-- 
<table>
	<tbody>
		<tr>
			<td width="800">
	<h4> UCSD, San Diego, USA  (Remote Collaboration) </h4>
	<ul>
	  <li>Position: Research Intern in <a  target="_blank" href=https://xiaolonw.github.io/ target="_blank" rel="external">Prof. Wang's Group</a></li>
		<li>Supervisor: Prof. <a  target="_blank" href=https://xiaolonw.github.io/ target="_blank" rel="external"> Xiaolong Wang </a></li>
	  <li>Project: Video Synthesis with Diffusion Models <b>Ongoing Project</b> </li>
  </ul>
</td>
<td>
	<img id="school_logo" src="./images/UCSD.png" height="100px">
</td>
</tr>
<tr></tr>
</tbody></table> -->

<!--    <img id="school_logo" src="./pics/uII.png">-->
<table>
	<tbody>
		<tr>
			<td width="800"> 
      <h4> TUM, Munich, Bayern, Germany  (Mar. 2022 - Aug. 2022) </h4>
	  <ul>
        <li>Position: Guided Research in <a  target="_blank" href=https://www.3dunderstanding.org target="_blank" rel="external">3D AI Group</a></li>
        <li>Supervisor: Prof. <a  target="_blank" href=https://www.3dunderstanding.org/team.html target="_blank" rel="external"> Angela Dai </a></li>
        <li>Project: Single-View Reconstruction and Category-level NeRF</li>
    </ul>
</td>
<td>
	<img id="school_logo" src="./images/TUM.jpg" height="100px">
</td>
</tr>
<tr></tr>
</tbody></table>

<!--    <img id="school_logo" src="./pics/Flexiv.png">-->
          <!--<h4> The University of North Carolina at Chapel Hill, NC, USA  & <br> Shanghai United ImagingIntelligence Co., Ltd, China  (Jun. 2018 - Nov. 2018)</br> </h4>-->
		  <table>
			<tbody>
				<tr>
					<td width="800"> 
		  <h4> TUM, Munich, Bayern, Germany  (Aug. 2021 - Mar. 2022) </h4>
          <ul>
            <li>Position: Research Praktikum in <a  target="_blank" href="https://www.niessnerlab.org/" target="_blank" rel="external">Visual Computing Group</a></li>
            <li>Supervisor: Prof.<a  target="_blank" href=https://www.niessnerlab.org/members/matthias_niessner/profile.html target="_blank" rel="external"> Matthias Niessner</a> and
				M.Sc <a  target="_blank" href=https://aljazbozic.github.io/ target="_blank" rel="external"> Alja≈æ Bo≈æiƒç </a>
			</li>
            <li>Project: Unsupervised Multi-View Stereo --> <b>ECCV 2022</b> </li>
            <!-- <br/>
            <br/> -->
            <!-- <td width="306">
            <img src="pics/masage.gif" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">
            </td> -->
      </ul>
	</td>
	<td>
		<img id="school_logo" src="./images/TUM.jpg" height="100px">
	</td>
	</tr>
	<tr></tr>
	</tbody></table>
<!--    <img id="school_logo" src="./pics/uII.png">-->
<table>
	<tbody>
		<tr>
			<td width="800">
      <h4> HKUST, Hong Kong, China  (Mar. 2021 - Sep. 2021) </h4>
	  <ul>
        <li>Position: Research Intern in <a  target="_blank" href="https://www.danxurgb.net/index.html" target="_blank" rel="external">HKUST Vision Group (MM-Lab at HKUST)</a></li>
        <li>Supervisor: Prof.<a  target="_blank" href=https://www.danxurgb.net/index.html target="_blank" rel="external"> Dan Xu</a>
        and M.Sc <a  target="_blank" href=https://mizhenxing.github.io/ target="_blank" rel="external"> Zhenxing Mi </a></li>
        <li>Project: Multi View Stereo Depth Estimation --> <b>CVPR 2022</b>  </li>
    </ul>
</td>
<td>
	<img id="school_logo" src="./images/HKUST.jpg" height="100px">
</td>
</tr>
<tr></tr>

</tbody></table>

<!-- <table>
		<tbody>
			<tr>
				<td width="800">
	<h4> DUT, Dalian, Liaoning, China  (Sep. 2020 - Feb. 2021) </h4>
	<ul>
	  <li>Position: Research Assistant in <a  target="_blank" href="https://ice.dlut.edu.cn/lu/index.html" target="_blank" rel="external">IIAU-Lab</a></li>
	  <li>Supervisor: Prof. Dong Wang and Prof. Huchuan Lu </li>
	  <li>Project: Underwater Robotics Vision <b><a href="https://arxiv.org/abs/2111.12982">Technical Report<a/></b></li>
  </ul>
</td>
<td>
	<img id="school_logo" src="./images/DUT.jpg" height="100px">
</td>
</tr>
<tr></tr>
</tbody></table> -->
<br>

<!-- <h2><font color="#CB4335">Scholarships</font></h2>
      <ul>
		<li>
          Aug 2019 <b>Annenberg Graduate Fellowship at University of Southern California</b></li>
        <li>
          Sep 2017 <b>National Scholarship(Graduate)</b>, (highest honor for graduates) <strong style="color:blue">top 1% nationwide</strong></li>
        <li>
          Sep 2015 <b>National Scholarship(Undergraduate)</b>, (highest honor for undergraduates, top 2% nationwide) </li>
		<li>
          May 2018 <b>KaiYuan Motivational Scholarship</b> <strong style="color:blue">top 0.5% in Shanghai Jiao Tong University</strong></li>
        <li>
          Sep 2015 <b>Presidential Scholarship</b>, (highest honor in Shandong University) <strong style="color:blue">top 0.2% in Shan Dong University</strong></li>
        <li>
          Sep 2015 <b>BaoGang Excellent student Scholarship</b>, (4 Places per year at Shandong University) </li>
		<li>
          Sep 2015 &amp; Sep 2014 &amp; Sep 2015</b> <b>First Prize Scholarship</b> (Top 6% in China,three-year continuous)</li>
      </ul>

<br> -->

<h2><font color="#CB4335">Students</font></h2>
      <ul>
		<li>
			<a  target="_blank" href=https://www.linkedin.com/in/jessica-fu-60a504254/ target="_blank" rel="external">Jessica Fu</a>. (Undergraduate student at USC, recruited through CURVE Fellowship. 2023 Fall)</li>		  
		<li>
			<a  target="_blank" href=https://www.linkedin.com/in/kevin-hopkins-1471781ba/ target="_blank" rel="external">Kevin Hopkins</a>. (Master student at USC. 2023 Fall)</li>
      </ul>

<br>

<h2><font color="#CB4335">Honors and Awards</font></h2>
      <ul>
        <!-- <li>
          Aug 2017 <b>The First Prize</b> 2017 ROBOMASTER <strong style="color:blue">The World‚Äôs Leading Robotics Competition</strong> (Responsible for the design
of electronic control in robotics)[<a  target="_blank" href="https://github.com/gyhandy/Andy">Code-T_Infantry</a>]</li>
		<li>
          Aug 2017 <b>Rank 1st</b> (preliminary competition) , Tianchi: Precision medical competition-Artificial Intelligence Aided
genetic risk prediction of diabetes [<a  target="_blank" href="https://github.com/gyhandy/Diabetes-Mellitus">Code-Pred_diabetes</a>]</li>
        <li>
          Oct 2015 <b>The First Prize</b> 9th international college students ican innovation and entrepreneurship contest</li> -->
		  
		<li>
			Mar 2022 <b>Outstanding UG graduates</b> of DUT </li>      
		<li>
			Oct 2021 <b>Outstanding Undergraduate</b> of DUT </li>     
        <li>
          May 2021 <b>The Third prize</b> 2021 China Underwater Robot Professional Contest - Acoustics Track </li>
		<li>
          May 2021 <b>The Second prize</b> 2021 China Underwater Robot Professional Contest - Optics Track </li>
		<li>
			Oct 2020 <b>Outstanding Undergraduate</b> of DUT </li>     
		<li>
			Oct 2019 <b>Outstanding Undergraduate</b> of DUT </li>        
      </ul>

<br>

<h2><font color="#CB4335">Academic Service</font></h2>
Reviewer of the following conferences/journals:
<br>
<br>
ECCV 2022<br>
NeurIPS 2022<br>

<!--2021 IEEE CVPR WORKSHOP ON FAIR, DATA EFFICIENT AND TRUSTED COMPUTER VISION<br>-->


<br>


<h2><font color="#CB4335">Teaching</font></h2>
      <ul>
		<li>
			Graduate Teaching Assistant, CSCI 103 Introduction to Programming - 2022 Fall</li>
        <!-- <li>
          Sep 2017 <b>National Scholarship(Graduate)</b>, (highest honor for graduates) <strong style="color:blue">top 1% nationwide</strong></li>
        <li>
          Sep 2015 <b>National Scholarship(Undergraduate)</b>, (highest honor for undergraduates, top 2% nationwide) </li>
		<li>
          May 2018 <b>KaiYuan Motivational Scholarship</b> <strong style="color:blue">top 0.5% in Shanghai Jiao Tong University</strong></li>
        <li>
          Sep 2015 <b>Presidential Scholarship</b>, (highest honor in Shandong University) <strong style="color:blue">top 0.2% in Shan Dong University</strong></li>
        <li>
          Sep 2015 <b>BaoGang Excellent student Scholarship</b>, (4 Places per year at Shandong University) </li>
		<li>
          Sep 2015 &amp; Sep 2014 &amp; Sep 2015</b> <b>First Prize Scholarship</b> (Top 6% in China,three-year continuous)</li> -->
      </ul>

<br>

<h2><font color="#CB4335">Miscellaneous</font></h2>
      <ul>
		<li>
			(10 yrs +) üé∑Clarinet, professional player, once level 9 certification of The Central Conservatory of Music.</li>		  
		<li>
			(8 yrs +) üè∏Badminton, I am a member of DUT EE Department badminton team.</li>
		<li>
			(5 yrs +) üéÆVideo Games, I am a lover of League of Legends, highest rank: Platinum II of EU server S12.</li>
      </ul>

<br>
  <!-- <h2><font color="#CB4335">Software & Patents</font></h2>
<table id="tbPublications" width="100%">


	<tr>
        <td width="306">
        <img src="pics/uspatent.png" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">
        </td>
        <td><strong>Systems and methods for image processing</strong><br>
            <em> S Liao, <b>GE Yunhao</b>, WEI Dongming<br></em>
            <em>US Patent App. 16/729,303</em>.
    <br>[<a href="https://ieeexplore.ieee.org/abstract/document/8413124/">Paper</a>]
        <br><br>
        </td>
    </tr>

    <tr>
        <td width="306">
        <img src="pics/software.jpg" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">
        </td>
        <td><strong>Pulmonary Nodular Assisted Detection System Based on AI(V1.0)</strong><br>
            <em> Bin Li, <b>Yunhao Ge</b><br></em>
            <em>2018SR037095</em>.
      <br>[<a href="https://ieeexplore.ieee.org/abstract/document/8413124/">Paper</a>]
        <br><br>
        </td>
    </tr>

    <tr>
        <td width="306">
        <img src="pics/patent1.jpg" width="285px" height= "170px" style="box-shadow: 4px 4px 8px #888">
        </td>
        <td><strong>A Two-Layer Barrier Free Parking Equipment Based on Bionic Manipulator</strong><br>
            <em><b>Yunhao Ge</b>, Shangze Yang, Zheng Zhang, Weixin Yan and Yanzheng Zhao <br></em>
            <em>CN201610712048</em>.
        <br>[<a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w3/Xu_Dual-Mode_Vehicle_Motion_CVPR_2018_paper.pdf">Paper</a>]
        <br><br>
        </td>
    </tr>

    <tr>
        <td width="306">
        <img src="pics/patent2.png" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">
        </td>
        <td><strong>A Double Decker Parking Equipment based on Shear Lifting Mechanism and Hydraulic Mechanism</strong><br>
            <em><b>Yunhao Ge</b>, Xulong Zhou, Peng Liu and Yanzheng Zhao <br></em>
            <em>CN201610704408</em>.
       <br>[<a href="https://github.com/gyhandy/publication/blob/master/A%20Real-time%20Gesture%20Prediction%20System%20Using%20Neural%20Networks%20and%20Multimodal%20Fusion%20based%20on%20data%20glove.pdf">Paper</a>] 
        <br><br>
        </td>
    </tr>
</table>
<br> -->


<!--
 <h3>Academice Service</h3>
<hr>
<table id="tbActivities" border="0" width="100%">
	Reviewer of the following conferences/journals:
    <p>IEEE Transactions on Multimedia (TMM) </p>
    <p>IEEE International Conference on Computer Vision (ICCV 2017)</p>
    <p>ACM Multimedia Conference (MM 2017)</p>
    <p>IEEE International Conference on Image Processing (ICIP 2017)</p>
    <p>AAAI Conference on Artificial Intelligence (AAAI 2016)</p>
</table> 
-->

<!-- <h2>OpenCourse Achievements</h2>-->
<!--      <h4> DeepLearning.ai</h4>-->
<!--      <ul>-->
<!--        <li>Neural Networks and Deep Learning</li>-->
<!--        <li>Improving Deep Neural Networks</li>-->
<!--        <li>Structuring Machine Learning Projects </li>-->
<!--		<li>Convolutional Neural Networks </li>-->
<!--  </ul> -->
<!--<br> -->
<!--
<h4>Links</h4>
<strong>
<a href="http://cs231n.stanford.edu/">CS231n@Stanford</a><br>
<a href="http://pytorch.org/">PyTorch</a><br>

</strong>
-->
<!-- <hr>

<script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5vzgzeo495p&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script> -->
<!-- <table width="35%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
	<tr>
	  <td>
		  <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=NLKW-iIrZCVUROMhdhzyEoRIQb66QjiOKeKjaB5DotM&cl=ffffff&w=a"></script>
	  </td>
	</tr>
  </tbody></table> -->



<div id="footer1">
	<h2> </h2>
		<div align="center">
		  <small>This page has been visited for
			<a href="https://www.easycounter.com/">
			<img src="https://www.easycounter.com/counter.php?dichang" border="0" alt="HTML Hit Counter"></a>times</small>
		  </div> 
	</div>
  <p>
	<center>
	<div align="center" style="width:20%">
	  <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=grr8tSJTdsbdU-vHO6Of-5W7jpLTZvSVYTu6BUgf02M"></script>
	</div>        
	</center>
  </p>
  

<p align="center"><font color="#999999">Last update: Apr. 10, 2022</font></p>



</body>

</html>

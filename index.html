<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
    <head>
    <meta name="google-site-verification" content="eoPCGBBxDIK0Ff9Dk_dXsuHMTNzzSEZMbsfO4zriBK8" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <link rel="shortcut icon" href="./images/doge.ico">
	<meta name="keywords" content="常迪, Chang Di, 常迪, CHANG Di, 常迪, Di CHANG, University of Southern California, USC, Technical University of Munich, TUM，Dalian University of Technology，DLUT, DUT, 大连理工大学, 慕尼黑工业大学, 南加州大学, München, Munich, 慕尼黑, California, 加利福尼亚州">
	<meta name="description" content="Di CHANG&#39;s Home Page">
<!--    <link href="main.css" media="all" rel="stylesheet">-->
    <link rel="stylesheet" href="jemdoc.css" type="text/css">
    <title>Di CHANG | 常迪</title>
    </head>

<body>


<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">
					<h1 style="color:#FF0000">Di (Kilian) Chang</h1><h1>
					<h1 style="color:#FF0000">常迪</h1><h1>
				</h1></div>

				<h3>Incoming CS Ph.D.  @ <a href="https://www.usc.edu/" target="_blank">University of Southern California</a></h3>
				<h3>Research Intern @ <a href="https://www.epfl.ch/labs/ivrl/" target="_blank">École Polytechnique Fédérale de Lausanne IVRL</a></h3>
<!--                <h3>University of Southern California</h3>-->
				<p>
					<a href="https://www.cs.usc.edu/" target="_blank">Department of Computer Science</a> <br>
					<a href="https://viterbischool.usc.edu/" target="_blank">Viterbi School of Engineering</a><br>
					<a href="https://www.usc.edu/" target="_blank">University of Southern California</a> <br>

<!--					Rm B06, Hedco Neurosciences Building, 3641 Watt Way, Los Angeles, CA 90089-2520, USA <br>-->

					Email1: di dot chang at tum dot de <br>
					Email2: dichang at usc dot edu
				</p>
				<p> <a href="https://scholar.google.com.hk/citations?hl=en&user=68wkMTgAAAAJ" target="_blank"><img src="./pics/google_scholar3.png" height="40px" style="margin-bottom:-3px"></a>
					<a href="https://github.com/Boese0601" target="_blank"><img src="./pics/github_s.jpg" height="40px" style="margin-bottom:-3px"></a>
					<a href="https://twitter.com/DiChang10" target="_blank"><img src="./images/twitter.png" height="40px" style="margin-bottom:-3px"></a>
                    <a href="https://www.linkedin.com/in/di-chang-004784206/" target="_blank"><img src="./pics/LinkedIn2.png" height="40px" style="margin-bottom:-3px"></a>
                    <a href="files/CV_Di.pdf"><img src="./pics/cv2.png" height="40px" style="margin-bottom:-3px"></a>
					&nbsp &nbsp
					<!-- <a href="#C1">[<font <font size="3" color="#CB4335"><b>About Me</b></font>] </a> -->
					<a href="#C2">[<font size="3" color="#CB4335"><b>News</b></font>]</a>
					<a href="#edu">[<font size="3" color="#CB4335"><b>Education</b></font>]</a>
					<a href="#C3">[<font size="3" color="#CB4335"><b>Publications</b></font>]</a>
					<a href="#C4">[<font size="3" color="#CB4335"><b>Experience</b></font>]</a></li>
				</p>
			</td>
			<td>
				<img src="./images/dichang.png" border="0" width="230"><br>
<!--				<img src="pics/cover.png" border="0" width="540"><br>-->
			</td>
		</tr><tr>
	</tr></tbody>
</table>

<!--<style>-->
<!--ul-->
<!--{-->
<!--	list-style-type:none;-->
<!--	margin:0;-->
<!--	padding:0;-->
<!--}-->
<!--li a:hover {-->
<!--    background-color: #555;-->
<!--    color: white;-->
<!--}-->

<!--</style>-->


<!--   #0F73B6-->

<h2><a id="C1" ><font color="#CB4335">About Me</font></a></h2>
<p> <a href="https://www.cs.usc.edu/"><img src="./pics/USC_logo.png" height="100px" style="margin-bottom:-3px"></a>&nbsp &nbsp &nbsp
	<a href="https://en.dlut.edu.cn/"><img src="./images/DUT.jpg" height="100px" style="margin-bottom:-3px"></a>&nbsp &nbsp &nbsp
	<a href="https://www.tum.de/en/"><img src="./images/TUM.jpg" height="100px" style="margin-bottom:-3px"></a>&nbsp &nbsp &nbsp
	<a href="https://www.epfl.ch/en/"><img src="./images/EPFL.png" height="120px" style="margin-bottom:-3px"></a>&nbsp &nbsp &nbsp &nbsp
	<a href="https://hkust.edu.hk/home"><img src="./images/HKUST.jpg" height="100px" style="margin-bottom:-3px"></a>&nbsp &nbsp &nbsp
	<br>
	<br>
	<!-- <a href="https://www.microsoft.com/en-us/research/"><img src="./pics/microsoft_logo.jfif" height="95px" style="margin-bottom:-3px"></a>&nbsp &nbsp &nbsp &nbsp
	<a href="https://www.uii-ai.com/en/"><img src="./pics/UII_logo33.png" height="98px" style="margin-bottom:-3px"></a>&nbsp &nbsp &nbsp
	<a href="http://flexiv.com/"><img src="./pics/flexiv_logo.jfif" height="105px" style="margin-bottom:-3px"></a> -->

</p>
<p>I am an incoming Ph.D. Candidate in the Department of Computer Science at <a  href="https://www.cs.usc.edu/">University of Southern California</a> (USC) with Prof. <a  target="_blank" href=https://people.ict.usc.edu/~soleymani/ target="_blank" rel="external">Mohammad Soleymani</a> of <a  href="https://www.ihp-lab.org/">IHP-Lab</a>.
	I'm also currently a research intern at <a  target="_blank" href=https://www.epfl.ch/labs/ivrl/ target="_blank" rel="external">Image and Visual Representation Lab</a> working with Dr. <a  target="_blank" href=https://sites.google.com/view/tong-zhang rel="external"> Tong Zhang </a>
        and Prof. <a  target="_blank" href=https://people.epfl.ch/sabine.susstrunk?lang=en target="_blank" rel="external"> Sabine Süsstrunk </a>.
	I also work closely with Prof. <a  target="_blank" href=https://xiaolonw.github.io/ target="_blank" rel="external"> Xiaolong Wang </a> and Prof. <a  target="_blank" href=https://cseweb.ucsd.edu/~haosu/ target="_blank" rel="external"> Hao Su </a> from <a  target="_blank" href=https://ucsd.edu/ target="_blank" rel="external"> UC San Diego </a>.
</p>
	Before that, I got my B.Eng gegree in Eletronic Information Engineering at <a href="https://en.dlut.edu.cn/">Dalian University of Technology</a> and B.Sc degree in Informatics at <a href=https://www.tum.de/en/ target="_blank" >Technical University of Munich</a>.
	I'm honored to be advised by Prof. <a  target="_blank" href=https://www.niessnerlab.org/members/matthias_niessner/profile.html target="_blank" rel="external"> Matthias Niessner </a> and
	M.Sc <a  target="_blank" href=https://aljazbozic.github.io/ target="_blank" rel="external"> Aljaž Božič </a> during my praktikum at <a  target="_blank" href="https://www.niessnerlab.org/" target="_blank" rel="external">Visual Computing Group</a>,
	as well as Prof. <a  target="_blank" href=https://www.3dunderstanding.org/team.html target="_blank" rel="external"> Angela Dai </a> when I was a guided research student at <a  target="_blank" href="https://www.3dunderstanding.org/" target="_blank" rel="external">3D AI Group</a>.
	I spent a wonderful summer in 2021 with Prof. <a  target="_blank" href=https://www.danxurgb.net/index.html target="_blank" rel="external"> Dan Xu </a> at HKUST CSE.
</p>
	I'm interested in Computer vision, Multi-View Stereo and Neural Rendering. My current research focuses include:
<ul>
	  <li>3D Vision, especially Multi-View Geometry and Scene Reconstruction with Deep Learning approaches, including Multi-View Depth Estimation with Cost Volume and Monocular Depth Estimation. </li>
	  <li>Pose Estimation, especially category-level object self-supervised pose estimation.</li>
	  <li>Graphics and Neural Radiance Field, especially category-level single-view unsupervised reconstruction and scene understanding.</li>
<!--	  <li>Effortless AI (how generative models reduce human effort and boost discriminative models)</li>-->
	</ul>
<!--<ul>-->
<!--	  <li>Causal Explainable AI ((1) Understanding reasoning logic and causality of Neural Networks (NN) (2) Use explanation as feedback to help improve the performance of the original NN.) </li>-->
<!--	  <li>Interpretable human-AI interaction (understanding AI models beyond accuracy, such as disentangled representation learning, human-NN knowledge exchange, steerability, generalization, fairness and bias)</li>-->
<!--	  <li>Humanoid Neural Network (simulating human cognitive learning ability (Imagination, Reasoning, Visual Recognition) by using various learning algorithms (Generative models, Representation Learning, Graph Neural Network, Contrastive Learning, etc.)</li>-->
<!--	  <li>Effortless AI (how generative models reduce human effort and boost discriminative models)</li>-->
<!--	</ul>-->

<!--	I'm interested in Machine Learning, Computer vision, and their applications towards Artificial General Intelligence (AGI). My current research focuses include:-->
<!--<ul>-->
<!--	  <li>interpretable human-AI interaction (interpretability, steerability, disentangled representation learning)</li>-->
<!--	  <li>generative models (data augmentation, how generative models boost discriminative models)</li>-->
<!--	  <li>graph neural networks (structure and relationship learning)</li>-->
<!--	  <li>visual reasoning, attention and saliency (cognitive learning, eye tracking)</li>-->
<!--	</ul>-->
<!--<p>My research interests lie in Machine Learning, Computer vision, and AGI. Currently, I am focusing on simulating Cognitive Baby Learning (Imagination, Reasoning, Attention)-->
<!--by using various learning algorithms (Representation Learning, Generative models, GNN, Reinforcement Learning,  Meta-Learning, etc.).</p>-->
<p> <b>Research opportunities</b>: I am happy to collaborate and/or answer questions about my research and my previous study at TUM and current PhD program at USC. If you are interested in research collaboration or have any inquiries about my experience, please send me an email.
	I especially encourage undergraduate students from DUT who want to work on interesting vision projects targeting top-tier conferences/journals to reach out.
</p>
<!--<p>My research interests lie in Computer vision, Robotics and General AI. Currently, I am-->
<!--focusing on simulating baby learning (Reasoning, Attention, Imagination) by using various learning algorithms (Representation Learning, Adversarial Learning,-->
<!--Meta Learning, GNN, Reinforcement Learning, etc.).</p>-->
<br>




<h2><a id="C2" ><font color="#CB4335">News</font></a></h2>
<ul>

<div style="height:200px;width:fit-content;overflow:auto;background:#FFFFFF;">
	<li>
		<p>[2022/04/08]  I'm joining <a href=https://web.northeastern.edu/smilelab/ target=_blank rel=noopener></a><a href=https://www.ihp-lab.org/ target=_blank rel=noopener>IHP-Lab</a> at <a href=https://www.northeastern.edu/ target=_blank rel=noopener></a><a href=https://www.cs.usc.edu/ target=_blank rel=noopener>USC</a> as a PhD student, see you in Los Angeles!</p>
	</li>
	<li>
		<p>[2022/03/15] Our <a href=https://mizhenxing.github.io/gbinet/ target=_blank rel=noopener>GBi-Net</a> has been accepted by CVPR 2022!</p>
	<li>
		<p>[2022/03/07] I will join <a href=https://www.3dunderstanding.org target=_blank rel=noopener>3D AI Lab</a> at <a href=https://www.tum.de/en/ target=_blank rel=noopener>Technische Universität München</a> as a guided research student with Professor Angela Dai.</p>
	</li>
	<li>
		<p>[2022/01/15] I will join <a href=https://www.epfl.ch/labs/ivrl/ target=_blank rel=noopener>Image and Visual Representation Lab</a> at <a href=https://www.epfl.ch/en/ target=_blank rel=noopener>École Polytechnique Fédérale de Lausanne</a> as a research intern.
	</li>
	<li>
		<p>[2021/10/15] I will join <a href=https://niessnerlab.org/ target=_blank rel=noopener>Visual Computing & Artificial Intelligence Lab</a> at <a href=https://www.tum.de/en/ target=_blank rel=noopener>Technische Universität München</a> with Professor Matthias Niessner.
	</li>
	<li>
		<p>[2021/03/04] I will join <a href=https://www.danxurgb.net/index.html target=_blank rel=noopener>Prof.Xu&rsquo;s vision group</a> at <a href=https://hkust.edu.hk/home target=_blank rel=noopener>HKUST</a> as a research intern during summer 2021.
	</li>
	
	<!-- <li>
		<p>[2021/05/17] I will be joining Computer Vision Group at <a  target="_blank" href=https://www.microsoft.com/en-us/research/ target="_blank" rel="external"> Microsoft Research </a>  Redmond as a research intern in summer 2021, advised by
        Dr. <a  target="_blank" href=http://vibhavvineet.info/ target="_blank" rel="external"> Vibhav Vineet </a>
        and Dr. <a  target="_blank" href=https://neelj.com/ target="_blank" rel="external"> Neel Joshi </a></li></p>
	</li>
	<li>
		<p>[2021/05/08] Serving as a reviewer for NeurIPS 2021 and ICLR 2022!</p>
	</li>
	<li>
		<p>[2021/04/07] Releasing <a  target="_blank" href=http://ilab.usc.edu/datasets/i2sg target="_blank" rel="external">Img2SceneGraph</a>,
		a pipeline that transfers images to scene graphs with node attributes!
		Welcome to <a  target="_blank" href=http://ilab.usc.edu/datasets/i2sg target="_blank" rel="external"> Download </a> and try!</p>
	</li>
	<li>
		<p>[2021/04/02] One paper (Graph Autoencoder for Graph Compression and Representation Learning)
		was accepted by Neural Compression Workshop @<b>ICLR</b> 2021 as <strong style="color:blue">Spotlight</strong>!</p>
	</li>
	<li>
		<p>[2021/02/28] One paper (A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts)
		was accepted by <b>CVPR</b> 2021!</p>
	</li>
	<li>
		<p>[2021/01/16] One paper (Beneficial Perturbation Network for designing general adaptive artificial intelligence systems)
		was accepted by <b>TNNLS</b>!</p>
	</li>
	<li>
		<p>[2021/01/12] One paper (Zero-shot Synthesis with Group-Supervised Learning) was accepted by <b>ICLR</b> 2021!</p>
	</li>
	<li>
		<p>[2020/09/14] <a  target="_blank" href=http://ilab.usc.edu/datasets/fonts target="_blank" rel="external"> Fonts dataset </a> was proposed for fast testing and idea iteration on disentangled representation learning and zero-shot synthesis.
			Welcome to <a  target="_blank" href=http://ilab.usc.edu/datasets/fonts target="_blank" rel="external"> Download </a> and try!</p>
	</li>
	<li>
		<p>[2020/07/02] One paper (Pose Augmentation: Class-agnostic Object Pose Transformation) was accepted by <b>ECCV</b> 2020!</p>
	</li>
	<li>
		<p>[2020/05/12] I will be joining UII America as a research intern in summer 2020, advised by
        Dr. <a  target="_blank" href=http://wuziyan.com/ target="_blank" rel="external"> Ziyan Wu </a>
        and Dr. <a  target="_blank" href=https://karanams.github.io/ target="_blank" rel="external"> Srikrishna Karanam </a></li></p>
	</li>
	<li>
		<p>[2019/08/12] I will be joining USC CS Ph.D. Program in fall 2019, advised by
        Prof. <a  target="_blank" href=http://ilab.usc.edu/itti/ target="_blank" rel="external">Laurent Itti</a>.</p>
	</li>
	<li>
		<p>[2019/07/01] One paper (Synthesis and inpainting-based MR-CT registration) was accepted by <b>MICCAI</b> 2019.</p>
	</li>
	<li>
		<p>[2019/03/01] One paper (Unpaired Whole-Body Mr to CT Synthesis) was accepted by <b>ISBI</b> 2019.</p>
	</li> -->

</div>
</ul>
<br>



<h2><a id="edu" ><font color="#CB4335">Education</font></a></h2>
<table>
	<tbody>
		<tr>
			<td width="800">
 
			<h4> University of Southern California, Los Angeles, USA <br>(Aug. 2022 - ???) (Not sure if I can finish my PhD)</h4>
			<ul>
				<li>
				<b>Doctor of Philosophy in Computer Science</b></li>
				<li>Major Orientation: Deep Learning for 3D Vision and Affective Computing </li>
			</ul>
			</td>
<td>
	<img id="school_logo" src="./images/usc_cs_logo.jpeg" height="100px">
</td>
</tr>
<tr></tr>
</tbody></table>



<table>
	<tbody>
		<tr>
			<td width="800">
 
 <h4> Technical University of Munich, Munich, Germany (Aug. 2021 - Jul. 2022)</h4>
 <ul>
	<li>
	  <b>Bachelor of Science in Informatics</b></li>
	<li>Major Orientation: Deep Learning for 3D Perception, 3D Scanning and Spatial Learning </li>
	<li>Overall GPA: 1.4/1.0 </li>
	<li>Bachelor thesis: "Supervised and Unsupervised Multi View Stereo for Depth Inference"</li>
 </ul>
</td>
<td>
	<img id="school_logo" src="./images/TUM.jpg" height="100px">
</td>
</tr>
<tr></tr>
</tbody></table>

<table>
<tbody>
	<tr>
		<td width="800">
 <h4> Dalian University of Technology, Dalian, China (Sep. 2018 - Jul. 2021)</h4>
 <ul>
	<li>
	  <b>Bachelor of Engineering in Eletronic Information Engineering</b></li>
	<li>Major Orientation: Object Detection and Tracking</li>
	<li>Overall GPA: 91.5/100 | 3.93/4.0</li>
 </ul>
</td>
<td>
	<img id="school_logo" src="./images/DUT.jpg" height="100px">
</td>
</tr>
<tr></tr>
</tbody></table>


<h2><a id="C3" ><font color="#CB4335">Selected Publications</font></a> [<a href=https://scholar.google.com.hk/citations?hl=en&user=68wkMTgAAAAJ>Google Scholar</a>]</h2>

<table id="tbPublications" width="100%">


<!-- 
	<tr>
		<td width="306">
		<img src="pics/VisualReasoning.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td>
			<b>A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts</b><br>
			<p style="color:#2E86C1 ";> <b>A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts</b> </p><br>
		<br>
			<b>TL;DR:</b> Takes a step towards mimicking the reasoning process of NNs and provide logical, concept-level
						explanations for final model decisions. <br>
			<br>
			<b>Yunhao Ge</b>, Yao Xiao, Zhi Xu, Meng Zheng, Srikrishna Karanam, Terrence Chen, Laurent Itti and Ziyan Wu  <br>
		<em>IEEE/ CVF International Conference on Computer Vision and Pattern Recognition </em>(<i><b>CVPR</b></i>), 2021.
		<p></p>
			<p>[<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ge_A_Peek_Into_the_Reasoning_of_Neural_Networks_Interpreting_With_CVPR_2021_paper.pdf" target="_blank">paper</a>]
			[<a href="https://github.com/gyhandy/Visual-Reasoning-eXplanation" target="_blank">github</a>]
			[<a href="http://ilab.usc.edu/andy/vrx" target="_blank">website</a>]
			[<a href="https://youtu.be/ZzkpUrK-cRA" target="_blank">video</a>]
			[<a href="https://zhuanlan.zhihu.com/p/380264276" target="_blank">知乎</a>]
			[<a href="https://mp.weixin.qq.com/s/FhQsi7twHkGskcE5fshOxA" target="_blank">机器之心</a>]
			[<a href="https://mp.weixin.qq.com/s/DqYlLVMggNEWuN76-OXJmQ" target="_blank">AI科技评论</a>]
			</p>
		</td>
	</tr>




	<tr>
		<td width="306">
		<img src="pics/GSL3.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td> <b>Zero-shot Synthesis with Group-Supervised Learning</b> <br>
		<br>
		<b>TL;DR:</b> Using Controllable disentangled representation learning to simulate human knowledge factorization for imagination <br>
			<br>
		<b>Yunhao Ge</b>, Sami Abu-El-Haija, Gan Xin and Laurent Itti  <br>
		<em>International Conference on Learning Representations </em>(<i><b>ICLR</b></i>), 2021.
		<p></p>
		<p>[<a href="https://arxiv.org/pdf/2009.06586.pdf" target="_blank">paper</a>]
			[<a href="https://github.com/gyhandy/Group-Supervised-Learning" target="_blank">code</a>]
			[<a href="http://sami.haija.org/iclr21gsl" target="_blank">website</a>]
			[<a href="http://ilab.usc.edu/datasets/fonts" target="_blank">Fonts Dataset</a>]
			[<a href="https://viterbischool.usc.edu/news/2021/07/usc-researchers-enable-ai-to-use-its-imagination/" target="_blank">USC Viterbi Press</a>]
			[<a href="https://zhuanlan.zhihu.com/p/364895887" target="_blank">知乎</a>]
			[<a href="https://mp.weixin.qq.com/s/o2HBYf3NF3UsMxUqkASdyg" target="_blank">AI科技评论</a>]<br>
			[<a  target="_blank" href=https://viterbischool.usc.edu/news/2021/07/usc-researchers-enable-ai-to-use-its-imagination/ target="_blank" rel="external"> USC News </a>]
			[<a  target="_blank" href=https://techxplore.com/news/2021-07-enabling-artificial-intelligence.html target="_blank" rel="external"> Tech Xplore </a>]
			[<a  target="_blank" href=https://www.technologynetworks.com/informatics/news/enabling-ai-to-use-its-imagination-350886 target="_blank" rel="external"> Technology Networks </a>]
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>

			<tr>
		<td width="306">
		<img src="pics/CIR.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td>
			<b>Encouraging Disentangled and Convex Representation with Controllable Interpolation Regularization</b><br>
			<p style="color:#2E86C1 ";> <b>A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts</b> </p><br>
		<br>
		<b>TL;DR:</b> A simple yet efficient method: Controllable Interpolation Regularization (CIR), which creates a positive loop where the disentanglement and convexity can help each other  <br>
			<br>
			<b>Yunhao Ge</b>, Zhi Xu, Yao Xiao, Gan Xin, Yunkui Pang and Laurent Itti <br>
		<em>arXiv, 2021.
		<p></p>
		<p>[<a href="https://arxiv.org/pdf/2112.03163.pdf" target="_blank">paper</a>]</p>
		</td>
	</tr>

	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>




		<tr>
		<td width="306">
		<img src="pics/GraphAE.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td>
			<b>Graph Autoencoder for Graph Compression and Representation Learning</b><br>
			<p style="color:#2E86C1 ";> <b>A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts</b> </p><br>
		<br>
		<b>TL;DR:</b> Multi-kernel Inductive Attention Graph Autoencoder (MIAGAE) utilizes the node similarity and graph structure to compress all nodes and edges as a whole.  <br>
			<br>
			<b>Yunhao Ge<sup>*</sup></b>, Yunkui Pang<sup>*</sup>, Linwei Li and Laurent Itti (*=equal contribution)<br>
		<em>Neural Compression: From Information Theory to Applications--Workshop@ </em>(<i><b>ICLR</b></i>), 2021.
		<p></p>
		<p>[<a href="https://openreview.net/pdf?id=Bo2LZfaVHNi" target="_blank">paper</a>]
			[<a href="https://github.com/Pangyk/Graph_AE" target="_blank">code</a>]
			[<a href="http://ilab.usc.edu/datasets/i2sg" target="_blank">Img2SceneGraph</a>]</p>
			<p><strong style="color:blue">Spotlight Presentation</strong></p>
		</td>
	</tr>


    <tr>
		<td width="306">
		<img src="pics/eccv.gif" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td> <b>Pose Augmentation: Class-agnostic Object Pose Transformation for Object Recognition</b> <br>
			<br>
			<b>TL;DR:</b> Eliminate-add generator to explicitly disentangle pose from object identity by maximizing pose entropy<br>
			<br>
		<b>Yunhao Ge</b>, Jiaping Zhao, Laurent Itti  <br>
		<em>European Conference on Computer Vision</em> (<i><b>ECCV</b></i>), 2020.
		<p></p>
		<p>[<a href="https://arxiv.org/pdf/2003.08526.pdf" target="_blank">paper</a>]
			[<a href="https://github.com/gyhandy/Pose-Augmentation" target="_blank">github</a>]
			[<a href="https://youtu.be/WHAFj9KXRFY" target="_blank">video-1min</a>]
			[<a href="https://youtu.be/9N8eyOmCWh4" target="_blank">video-10min</a>]</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>

	<tr>
		<td width="306">
		<img src="pics/Beneficial.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td> <b>Beneficial Perturbation Network for designing general adaptive artificial intelligence systems</b> <br>
		<b>TL;DR:</b>Allowing a single network to learn potentially unlimited parallel input to output mappings, and to switch on the fly
			between them at runtime.<br>
			<br>
		Shixian Wen, Amanda Rios<sup>*</sup>, <b>Yunhao Ge<sup>*</sup></b> and Laurent Itti (*=equal contribution) <br>
		<em> IEEE Transactions on Neural Networks and Learning Systems </em>(<i><b>TNNLS</b></i>), 2021.
		<p></p>
		<p>[<a href="https://arxiv.org/pdf/2009.13954.pdf" target="_blank">paper</a>]
		</td>




    <tr>
		<td width="306">
		<img src="pics/project11.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td> <b>Unpaired MR to CT Synthesis with Explicit Structural Constrained Adversarial Learning</b> <br>
			 <br>
		<b>Yunhao Ge<sup>*</sup></b>, Dongming Wei<sup>*</sup>, Zhong Xue, Yiqiang Zhan, Xiang Zhou, Qian Wang and Shu Liao (*=equal contribution)<br>
		<em>IEEE International Symposium on Biomedical Imaging</em> (<i><b>ISBI</b></i>), 2019.
		<p></p>
		<p>[<a href="files/UNPAIRED.pdf" target="_blank">paper</a>]
            [<a href="https://github.com/gyhandy/Unpaired-Cross-modality-Image-Synthesis" target="_blank">code</a>]</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr> -->


    <tr>
		<td width="306">
		<img src="images/rc-mvsnet.JPG" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td> <b>RC-MVSNet: Unsupervised Multi-View Stereo with Neural Rendering</b> <br>
			<br>
			<b>Di Chang</b>, <a href=https://aljazbozic.github.io/ target=_blank rel=noopener>Aljaž Božic</a>, <a href="https://people.epfl.ch/tong.zhang?lang=en" target=_blank rel=noopener>Tong Zhang</a>, Qingsong Yan, <a href=https://www.yingcong.me/ target=_blank rel=noopener>Yingcong Chen</a>, <a href=https://people.epfl.ch/sabine.susstrunk target=_blank rel=noopener>Sabine Süsstrunk</a> and <a href=https://niessnerlab.org/ target=_blank rel=noopener>Matthias Nießner</a><br>
			<!-- <b>Di Chang</b>, <a>Aljaž Božič, Tong Zhang, Qingsong Yan, Yingcong Chen, Sabine Süsstrunk and Matthias Nießner <br> -->
		<em>Arxiv Preprint</em> (<i><b>Under Review</b></i>), 2022.
		<p></p>
		<p>[<a href="https://arxiv.org/abs/2203.03949" target="_blank">paper</a>]</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>

    <tr>
		<td width="306">
		<img src="images/gbinet.JPG" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td> <b>Generalized Binary Search Network for Highly-Efficient Multi-View Stereo</b> <br>
			<br>
		<a href="https://mizhenxing.github.io/">Zhenxing Mi</a>, <b>Di Chang</b> and <a href="https://www.danxurgb.net/index.html">Dan Xu</a> <br>
		<em>IEEE/ CVF International Conference on Computer Vision and Pattern Recognition</em> (<i><b>CVPR</b></i>), 2022.
		<p></p>
		<p>
			[<a href="https://mizhenxing.github.io/gbinet/" target="_blank">website</a>]
			[<a href="https://arxiv.org/abs/2112.02338" target="_blank">paper</a>]
            [<a href="https://github.com/MiZhenxing/GBi-Net" target="_blank">code</a>]
		</p>

		<!-- <p><strong style="color:blue">Oral Presentation</strong></p> -->
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>


<!--    <tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/project3.png" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td> <b>A Real-time Gesture Prediction System Using Neural Networks and Multimodal Fusion-->
<!--					based on Data Glove</b> <br>-->
<!--			<br>-->
<!--		<b>Yunhao Ge</b>, Bin Li and Weixin Yan <br>-->
<!--		<em>IEEE International Conference on Advanced Computational Intelligence</em> (<i><b>ICACI</b></i>), 2018.-->
<!--		<p></p>-->
<!--		<p>[<a href="files/Real.pdf" target="_blank">paper</a>]</p>-->
<!--			<p><strong style="color:blue">Oral Presentation</strong></p>-->
<!--		</td>-->
<!--	</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->

<!--    <tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/HHnet.png" width="285px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td> <b>HH-Net: Image driven microscope fast auto-focus with deep neural network</b><br>-->
<!--			<br>-->
<!--		<b>Yunhao Ge</b>, Bin Li, Yanzheng Zhao and Weixin Yan <br>-->
<!--		<em>International Conference on Biomedical Engineering and Technology</em> (<i><b>ICBET</b></i>), 2019.-->
<!--		<p></p>-->
<!--		<p>[<a href="files/HH-net.pdf" target="_blank">paper</a>]-->
<!--        </p>-->
<!--		</td>-->
<!--	</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->

<!--    <tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/project4.png" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td> <b>Melanoma Segmentation and Classification in Clinical Images Using Deep Learning</b><br>-->
<!--			<br>-->
<!--		<b>Yunhao Ge</b>, Bin Li and Weixin Yan <br>-->
<!--		<em>ACM International Conference on Machine Learning and Computing</em> (<i><b>ICMLC</b></i>), 2018.-->
<!--		<p></p>-->
<!--		<p>[<a href="files/Melanoma.pdf" target="_blank">paper</a>][<a  target="_blank" href="https://dl.acm.org/citation.cfm?id=3195164">Paper Link</a>]-->
<!--        </p>-->
<!--		</td>-->
<!--	</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->

<!--    <tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/project5.png" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td> <b>Benign and Malignant Mammographic Image Classification based on Convolutional-->
<!--					Neural Networks</b><br>-->
<!--			<br>-->
<!--		Bin Li, <b>Yunhao Ge</b>, Yanzheng Zhao, Enguang Guan and Weixin Yan <br>-->
<!--		<em>ACM International Conference on Machine Learning and Computing</em> (<i><b>ICMLC</b></i>), 2018.-->
<!--		<p></p>-->
<!--		<p>[<a href="files/Benign.pdf" target="_blank">paper</a>][<a  target="_blank" href="https://dl.acm.org/citation.cfm?id=3195163&dl=ACM&coll=DL">Paper Link</a>]-->
<!--        </p>-->
<!--		</td>-->
<!--	</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->

<!--    <tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/project.png" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td> <b>Effect of Mechanical Error on Dual-Wedge Laser Scanning System and Error Correction</b><br>-->
<!--			<br>-->
<!--		<b>Yunhao Ge</b>, Jihao Liu, Fenfen Xue, Enguang Guan, Weixin Yan and Yanzheng Zhao <br>-->
<!--		<i><b>Applied Optics</b></i>, 2018.-->
<!--		<p></p>-->
<!--		<p>[<a href="files/Effect.pdf" target="_blank">paper</a>][<a  target="_blank" href="https://www.osapublishing.org/ao/abstract.cfm?uri=ao-57-21-6047">Paper Link</a>]-->
<!--        </p>-->
<!--		</td>-->
<!--	</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->

<!--    <tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/project8.png" width="285px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td> <b>Dynamic Drive Performances of the Bionic Suction Cup Actuator Based on Shape Memory Alloy</b><br>-->
<!--		<b>Yunhao Ge</b>, Jihao Liu, Bin Li, Weixin Yan and Yanzheng Zhao  <br>-->
<!--		<em>Intelligent Robotics and Applications</em> (<i><b>ICIRA</b></i>), 2017.-->
<!--		<p></p>-->
<!--		<p>[<a href="files/Dynamic.pdf" target="_blank">paper</a>][<a  target="_blank" href="https://link.springer.com/chapter/10.1007%2F978-3-319-65289-4_2">Paper Link</a>]-->
<!--        </p>-->
<!--		</td>-->
<!--	</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->


<!--	<tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/project7.png" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td><strong>Interactive Experience across Modern Age and Tradition: Application of Arduino in Shadow Play</strong><br>-->
<!--            <em>Kun Yin, <b>Yunhao Ge</b>, Heshan Liu and Yongquan Yin  <br></em>-->
<!--            <em>Advances in Mechatronics and Machinery </em>.-->
<!--			<br>[<a target="_blank" href="files/Interactive.pdf">PDF</a>] [<a  target="_blank" href="https://www.scientific.net/AMM.868.242">Paper Link</a>]-->
<!--&lt;!&ndash;		<br>[<a  target="_blank" href="https://www.scientific.net/AMM.868.242">Paper Link</a>]&ndash;&gt;-->
<!--        <br><br>-->
<!--		</td>-->
<!--	</tr>-->



</table>
<br>
<h2><a id="C4" ><font color="#CB4335">Intern & Work Experience</font></a></h2>


<table>
	<tbody>
		<tr>
			<td width="800">
<h4>EPFL, Lausanne, Switzerland (Jul. 2022 - Oct. 2022) </h4>
	  <ul>
        <li>Position: Summer@EPFL (Top 2% from the candidates in the world) hired by <a  target="_blank" href=https://www.epfl.ch/labs/ivrl/ target="_blank" rel="external">IVRL</a></li>
		  <li>Supervisor: Dr. <a  target="_blank" href=https://sites.google.com/view/tong-zhang rel="external"> Tong Zhang </a>
			and Prof. <a  target="_blank" href=https://people.epfl.ch/sabine.susstrunk?lang=en target="_blank" rel="external"> Sabine Süsstrunk</a></li>
        <li>Project: Video MVS and Structure from Motion --> <b>Ongoing Project</b> </li>
    </ul>
</td>
<td>
	<img id="school_logo" src="./images/EPFL.png" height="100px">
</td>
</tr>
<tr></tr>
</tbody></table>

<table>
	<tbody>
		<tr>
			<td width="800">
	<h4> UCSD, San Diego, USA  (Remote Collaboration) </h4>
	<ul>
	  <li>Position: Research Intern in <a  target="_blank" href=https://cseweb.ucsd.edu/~haosu/ target="_blank" rel="external">Prof. Su's Lab</a></li>
		<li>Supervisor: Prof. <a  target="_blank" href=https://cseweb.ucsd.edu/~haosu/ target="_blank" rel="external"> Hao Su </a></li>
	  <li>Project: Deep Learning for Multi-View Geometry: A Survey </li>
  </ul>
</td>
<td>
	<img id="school_logo" src="./images/UCSD.png" height="100px">
</td>
</tr>
<tr></tr>
</tbody></table>
<!--    <img id="school_logo" src="./pics/uII.png">-->
<table>
	<tbody>
		<tr>
			<td width="800"> 
      <h4> TUM, Munich, Bayern, Germany  (Mar. 2022 - Present) </h4>
	  <ul>
        <li>Position: Guided Research in <a  target="_blank" href=https://www.3dunderstanding.org target="_blank" rel="external">3D AI Group</a></li>
        <li>Supervisor: Prof. <a  target="_blank" href=https://www.3dunderstanding.org/team.html target="_blank" rel="external"> Angela Dai </a></li>
        <li>Project: Single-View Reconstruction and Category-level NeRF --> <b>Ongoing Project</b> </li>
    </ul>
</td>
<td>
	<img id="school_logo" src="./images/TUM.jpg" height="100px">
</td>
</tr>
<tr></tr>
</tbody></table>

<!--    <img id="school_logo" src="./pics/Flexiv.png">-->
          <!--<h4> The University of North Carolina at Chapel Hill, NC, USA  & <br> Shanghai United ImagingIntelligence Co., Ltd, China  (Jun. 2018 - Nov. 2018)</br> </h4>-->
		  <table>
			<tbody>
				<tr>
					<td width="800"> 
		  <h4> TUM, Munich, Bayern, Germany  (Aug. 2021 - Mar. 2022) </h4>
          <ul>
            <li>Position: Research Praktikum in <a  target="_blank" href="https://www.niessnerlab.org/" target="_blank" rel="external">Visual Computing Group</a></li>
            <li>Supervisor: Prof.<a  target="_blank" href=https://www.niessnerlab.org/members/matthias_niessner/profile.html target="_blank" rel="external"> Matthias Niessner</a> and
				M.Sc <a  target="_blank" href=https://aljazbozic.github.io/ target="_blank" rel="external"> Aljaž Božič </a>
			</li>
            <li>Project: Unsupervised Multi-View Stereo --> <b>submitted to ECCV 2022</b> </li>
            <!-- <br/>
            <br/> -->
            <!-- <td width="306">
            <img src="pics/masage.gif" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">
            </td> -->
      </ul>
	</td>
	<td>
		<img id="school_logo" src="./images/TUM.jpg" height="100px">
	</td>
	</tr>
	<tr></tr>
	</tbody></table>
<!--    <img id="school_logo" src="./pics/uII.png">-->
<table>
	<tbody>
		<tr>
			<td width="800">
      <h4> HKUST, Hong Kong, China  (Mar. 2021 - Sep. 2021) </h4>
	  <ul>
        <li>Position: Research Intern in <a  target="_blank" href="https://www.danxurgb.net/index.html" target="_blank" rel="external">HKUST Vision Group (MM-Lab at HKUST)</a></li>
        <li>Supervisor: Prof.<a  target="_blank" href=https://www.danxurgb.net/index.html target="_blank" rel="external"> Dan Xu</a>
        and M.Sc <a  target="_blank" href=https://mizhenxing.github.io/ target="_blank" rel="external"> Zhenxing Mi </a></li>
        <li>Project: Multi View Stereo Depth Estimation --> <b>CVPR 2022</b>  </li>
    </ul>
</td>
<td>
	<img id="school_logo" src="./images/HKUST.jpg" height="100px">
</td>
</tr>
<tr></tr>

</tbody></table>

	<table>
		<tbody>
			<tr>
				<td width="800">
	<h4> DUT, Dalian, Liaoning, China  (Sep. 2020 - Feb. 2021) </h4>
	<ul>
	  <li>Position: Research Assistant in <a  target="_blank" href="https://ice.dlut.edu.cn/lu/index.html" target="_blank" rel="external">IIAU-Lab</a></li>
	  <li>Supervisor: Prof. Dong Wang and Prof. Huchuan Lu </li>
	  <li>Project: Underwater Robotics Vision --> <b><a href="https://arxiv.org/abs/2111.12982">Technical Report<a/></b></li>
  </ul>
</td>
<td>
	<img id="school_logo" src="./images/DUT.jpg" height="100px">
</td>
</tr>
<tr></tr>
</tbody></table>
<br>

<!-- <h2><font color="#CB4335">Scholarships</font></h2>
      <ul>
		<li>
          Aug 2019 <b>Annenberg Graduate Fellowship at University of Southern California</b></li>
        <li>
          Sep 2017 <b>National Scholarship(Graduate)</b>, (highest honor for graduates) <strong style="color:blue">top 1% nationwide</strong></li>
        <li>
          Sep 2015 <b>National Scholarship(Undergraduate)</b>, (highest honor for undergraduates, top 2% nationwide) </li>
		<li>
          May 2018 <b>KaiYuan Motivational Scholarship</b> <strong style="color:blue">top 0.5% in Shanghai Jiao Tong University</strong></li>
        <li>
          Sep 2015 <b>Presidential Scholarship</b>, (highest honor in Shandong University) <strong style="color:blue">top 0.2% in Shan Dong University</strong></li>
        <li>
          Sep 2015 <b>BaoGang Excellent student Scholarship</b>, (4 Places per year at Shandong University) </li>
		<li>
          Sep 2015 &amp; Sep 2014 &amp; Sep 2015</b> <b>First Prize Scholarship</b> (Top 6% in China,three-year continuous)</li>
      </ul>

<br> -->

<h2><font color="#CB4335">Honors and Awards</font></h2>
      <ul>
        <!-- <li>
          Aug 2017 <b>The First Prize</b> 2017 ROBOMASTER <strong style="color:blue">The World’s Leading Robotics Competition</strong> (Responsible for the design
of electronic control in robotics)[<a  target="_blank" href="https://github.com/gyhandy/Andy">Code-T_Infantry</a>]</li>
		<li>
          Aug 2017 <b>Rank 1st</b> (preliminary competition) , Tianchi: Precision medical competition-Artificial Intelligence Aided
genetic risk prediction of diabetes [<a  target="_blank" href="https://github.com/gyhandy/Diabetes-Mellitus">Code-Pred_diabetes</a>]</li>
        <li>
          Oct 2015 <b>The First Prize</b> 9th international college students ican innovation and entrepreneurship contest</li> -->
		  
		<li>
			Mar 2022 <b>Outstanding UG graduates</b> of DUT </li>      
		<li>
			Oct 2021 <b>Outstanding Undergraduate</b> of DUT </li>     
        <li>
          May 2021 <b>The Third prize</b> 2021 China Underwater Robot Professional Contest - Acoustics Track </li>
		<li>
          May 2021 <b>The Second prize</b> 2021 China Underwater Robot Professional Contest - Optics Track </li>
		<li>
			Oct 2020 <b>Outstanding Undergraduate</b> of DUT </li>     
		<li>
			Oct 2019 <b>Outstanding Undergraduate</b> of DUT </li>        
      </ul>

<br>

<h2><font color="#CB4335">Academic Service</font></h2>
Reviewer of the following conferences/journals:
<br>
<br>
ECCV 2022<br>
NeurIPS 2022<br>

<!--2021 IEEE CVPR WORKSHOP ON FAIR, DATA EFFICIENT AND TRUSTED COMPUTER VISION<br>-->
ACM Transactions on Knowledge Discovery from Data (TKDD)<br>

<br>


<h2><font color="#CB4335">Teaching</font></h2>
      <ul>
		<li>
			Teaching Assistant, IN2346 Introduction to Deep Learning - Summer Semester 2022</li>
        <!-- <li>
          Sep 2017 <b>National Scholarship(Graduate)</b>, (highest honor for graduates) <strong style="color:blue">top 1% nationwide</strong></li>
        <li>
          Sep 2015 <b>National Scholarship(Undergraduate)</b>, (highest honor for undergraduates, top 2% nationwide) </li>
		<li>
          May 2018 <b>KaiYuan Motivational Scholarship</b> <strong style="color:blue">top 0.5% in Shanghai Jiao Tong University</strong></li>
        <li>
          Sep 2015 <b>Presidential Scholarship</b>, (highest honor in Shandong University) <strong style="color:blue">top 0.2% in Shan Dong University</strong></li>
        <li>
          Sep 2015 <b>BaoGang Excellent student Scholarship</b>, (4 Places per year at Shandong University) </li>
		<li>
          Sep 2015 &amp; Sep 2014 &amp; Sep 2015</b> <b>First Prize Scholarship</b> (Top 6% in China,three-year continuous)</li> -->
      </ul>

<br>


  <!-- <h2><font color="#CB4335">Software & Patents</font></h2>
<table id="tbPublications" width="100%">


	<tr>
        <td width="306">
        <img src="pics/uspatent.png" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">
        </td>
        <td><strong>Systems and methods for image processing</strong><br>
            <em> S Liao, <b>GE Yunhao</b>, WEI Dongming<br></em>
            <em>US Patent App. 16/729,303</em>.
    <br>[<a href="https://ieeexplore.ieee.org/abstract/document/8413124/">Paper</a>]
        <br><br>
        </td>
    </tr>

    <tr>
        <td width="306">
        <img src="pics/software.jpg" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">
        </td>
        <td><strong>Pulmonary Nodular Assisted Detection System Based on AI(V1.0)</strong><br>
            <em> Bin Li, <b>Yunhao Ge</b><br></em>
            <em>2018SR037095</em>.
      <br>[<a href="https://ieeexplore.ieee.org/abstract/document/8413124/">Paper</a>]
        <br><br>
        </td>
    </tr>

    <tr>
        <td width="306">
        <img src="pics/patent1.jpg" width="285px" height= "170px" style="box-shadow: 4px 4px 8px #888">
        </td>
        <td><strong>A Two-Layer Barrier Free Parking Equipment Based on Bionic Manipulator</strong><br>
            <em><b>Yunhao Ge</b>, Shangze Yang, Zheng Zhang, Weixin Yan and Yanzheng Zhao <br></em>
            <em>CN201610712048</em>.
        <br>[<a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w3/Xu_Dual-Mode_Vehicle_Motion_CVPR_2018_paper.pdf">Paper</a>]
        <br><br>
        </td>
    </tr>

    <tr>
        <td width="306">
        <img src="pics/patent2.png" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">
        </td>
        <td><strong>A Double Decker Parking Equipment based on Shear Lifting Mechanism and Hydraulic Mechanism</strong><br>
            <em><b>Yunhao Ge</b>, Xulong Zhou, Peng Liu and Yanzheng Zhao <br></em>
            <em>CN201610704408</em>.
       <br>[<a href="https://github.com/gyhandy/publication/blob/master/A%20Real-time%20Gesture%20Prediction%20System%20Using%20Neural%20Networks%20and%20Multimodal%20Fusion%20based%20on%20data%20glove.pdf">Paper</a>] 
        <br><br>
        </td>
    </tr>
</table>
<br> -->


<!--
 <h3>Academice Service</h3>
<hr>
<table id="tbActivities" border="0" width="100%">
	Reviewer of the following conferences/journals:
    <p>IEEE Transactions on Multimedia (TMM) </p>
    <p>IEEE International Conference on Computer Vision (ICCV 2017)</p>
    <p>ACM Multimedia Conference (MM 2017)</p>
    <p>IEEE International Conference on Image Processing (ICIP 2017)</p>
    <p>AAAI Conference on Artificial Intelligence (AAAI 2016)</p>
</table> 
-->

<!-- <h2>OpenCourse Achievements</h2>-->
<!--      <h4> DeepLearning.ai</h4>-->
<!--      <ul>-->
<!--        <li>Neural Networks and Deep Learning</li>-->
<!--        <li>Improving Deep Neural Networks</li>-->
<!--        <li>Structuring Machine Learning Projects </li>-->
<!--		<li>Convolutional Neural Networks </li>-->
<!--  </ul> -->
<!--<br> -->
<!--
<h4>Links</h4>
<strong>
<a href="http://cs231n.stanford.edu/">CS231n@Stanford</a><br>
<a href="http://pytorch.org/">PyTorch</a><br>

</strong>
-->
<!-- <hr>

<script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5vzgzeo495p&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script> -->
<!-- <table width="35%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
	<tr>
	  <td>
		  <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=NLKW-iIrZCVUROMhdhzyEoRIQb66QjiOKeKjaB5DotM&cl=ffffff&w=a"></script>
	  </td>
	</tr>
  </tbody></table> -->



<div id="footer1">
	<h2> </h2>
		<div align="center">
		  <small>This page has been visited for
			<a href="https://www.easycounter.com/">
			<img src="https://www.easycounter.com/counter.php?dichang" border="0" alt="HTML Hit Counter"></a>times</small>
		  </div> 
	</div>
  <p>
	<center>
	<div align="center" style="width:20%">
	  <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=grr8tSJTdsbdU-vHO6Of-5W7jpLTZvSVYTu6BUgf02M"></script>
	</div>        
	</center>
  </p>
  

<p align="center"><font color="#999999">Last update: Apr. 10, 2022</font></p>



</body>

</html>
